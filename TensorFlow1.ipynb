{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TensorFlow1.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPs+OOsut+EBoTi4w8J9Rb0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eFbHc9DOJA2S","colab_type":"code","colab":{}},"source":["hello = tf.constant(\"Hwllo, TensorFlow!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kgsgvs88JMnu","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuimzP5jJZXq","colab_type":"code","colab":{}},"source":["hello = tf.constant(\"Hwllo, TensorFlow!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4O5P-76JbBC","colab_type":"code","colab":{}},"source":["sess = tf.Session()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qp6IbL0OJgYs","colab_type":"code","colab":{}},"source":["print((hello))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtLnxrrtJ2MF","colab_type":"code","colab":{}},"source":["sess = tf.Session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdoIXv2eKjw7","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","hello = tf.constant(\"Hwllo, TensorFlow!\")\n","sess = tf.Session()\n","print(sess.run(hello))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMbahij-K7wS","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmhdewWeLL0u","colab_type":"code","colab":{}},"source":["!pip uninstall tensorflow\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCrZ0ECdLQoX","colab_type":"code","colab":{}},"source":["!pip install tensorflow==1.15"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KYCSkZbLbti","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","hello = tf.constant(\"Hello, TensorFlow!\")\n","sess = tf.Session()\n","print(sess.run(hello))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODj23DJSLxih","colab_type":"code","colab":{}},"source":["node1 = tf.constant(3.0,tf.float32)\n","node2 = tf.constant(4.0)\n","node3 = tf.add(node1,node2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KYf1nQ-MMtm","colab_type":"code","colab":{}},"source":["sess= tf.Session()\n","print(\"sess.run(node1,node2:\", sess.run([node1,node2]))\n","print(sess.run(node3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gJUgIF4MdWT","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","x_train = [1,2,3] #x가 1일때 y 가 1이되고 x가 2일때 y가 2가 되는 것 \n","y_train = [1,2,3] # 간단한 하나의 값들을 예측해 보라 \n","\n","W = tf.Variable(tf.random_normal([1]),name='weight') # Variable 은 텐서플로우가 자체적으로 변경하는 값이다.  학습하는 과정에서 값을 바꿔가며 트레인한다\n","b = tf.Variable(tf.random_normal([1]),name='bias') #  . W와 b 의 값을 모르기 때문에 중간에는 랜덤값을 넣어준다, 1은 shape(타입)으로서 값이 하나인 1차원 값을 넣어준다.\n","\n","#Our hypothesis XW+b\n","hypothesis = x_train *W + b\n","\n","#cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis -y_train)) #\n","\n","#Minmize\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n","train = optimizer.minimize(cost) # cost 를 minimize 이다  여기서 트레인이라는 것을 실행 시켜야 코스트를 미니마이즈하고 한다.\n","\n","#Launch the graph in a session\n","sess = tf.Session() # 실행하기 위해 세션을 만든다.\n","\n","#Initializes global variable in the graph.\n","sess.run(tf.global_variables_initializer()) # W와 b에 베리어블을 사용 하기위해서 여기서 global_variables_initializer 실행 시켜줘야 한다.\n","\n","#Fit the line\n","for step in range(2001): # 각스텝을 2001로 하고 \n","    sess.run(train)\n","    if step % 20 == 0: #20번에 한번씩 출력하게 끔 한다. cost와 w b 값을 확인한다.\n","        print(step,sess.run(cost), sess.run(W), sess.run(b))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ICJ_6H1xBLE","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","\n","W = tf.Variable(tf.random_normal([1]),name='weight') # Variable 은 텐서플로우가 자체적으로 변경하는 값이다.  학습하는 과정에서 값을 바꿔가며 트레인한다\n","b = tf.Variable(tf.random_normal([1]),name='bias') #  . W와 b 의 값을 모르기 때문에 중간에는 랜덤값을 넣어준다, 1은 shape(타입)으로서 값이 하나인 1차원 값을 넣어준다.\n","\n","X = tf.placeholder(tf.float32, shape = [None])\n","Y = tf.placeholder(tf.float32, shape = [None])\n","\n","#Our hypothesis XW+b\n","hypothesis = X *W + b\n","\n","#cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis -Y)) #\n","\n","#Minmize\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n","train = optimizer.minimize(cost) # cost 를 minimize 이다  여기서 트레인이라는 것을 실행 시켜야 코스트를 미니마이즈하고 한다.\n","\n","#Launch the graph in a session\n","sess = tf.Session() # 실행하기 위해 세션을 만든다.\n","\n","#Initializes global variable in the graph.\n","sess.run(tf.global_variables_initializer()) # W와 b에 베리어블을 사용 하기위해서 여기서 global_variables_initializer 실행 시켜줘야 한다.\n","\n","#Fit the line\n","for step in range(10000): # 각스텝을 2001로 하고\n","    cost_val,W_val, b_val, _ = sess.run([cost,W,b,train],\n","                                        feed_dict={X:[1,2,3,4,5],  #여기서 원하는 학습데이터를 넣어줬다. 여기서 x 가 독립변수\n","                                                   Y:[2.1, 3.1, 4.1, 5.1, 6.1]}) # 종속변수 \n","    if step % 20 == 0: #20번에 한번씩 출력하게 끔 한다. cost와 w b 값을 확인한다.\n","        print(step,cost_val,W_val, b_val )\n","\n","print(sess.run(hypothesis, feed_dict={X:[5]}))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zC2D5ph564LC","colab_type":"code","colab":{}},"source":["print(sess.run(hypothesis, feed_dict={X:[5]}))\n","print(sess.run(hypothesis, feed_dict={X:[2.5]}))\n","print(sess.run(hypothesis, feed_dict={X:[1.5,3.5]}))\n","\n","# 이부분은 지금 내가 만든 모델에 테스트 데이터를 넣어서 나오는 값이 나온다. 지금 모델이 1.1을 더한 값이여서 값이 이렇게 나온다"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xHtKn7S_tJq","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plot\n","\n","x=[1,2,3]\n","y=[1,2,3]\n","\n","W = tf.placeholder(tf.float32)\n","\n","hypothesis = X * W\n","\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","sess = tf.Session()\n","\n","sess.run(tf.global_variables_initializer())\n","\n","W_val = []\n","cost_val = []\n","\n","for i in range(-30,50):\n","    feed_W = i * 0.1\n","    curr_cost, curr_W = sess.run([cost,W], feed_dict={W:feed_W})\n","    W_val.append(curr_W)\n","    cost_val.append(curr_cost)\n","\n","plt.plot(W_val, cost_val)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdA14Md9BNZ_","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","X=[1,2,3]\n","Y=[1,2,3]\n","\n","W = tf.placeholder(tf.float32)\n","\n","hypothesis = X * W\n","\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","sess = tf.Session()\n","\n","sess.run(tf.global_variables_initializer())\n","\n","\n","W_val = []\n","cost_val = []\n","\n","for i in range(-30,50):\n","    feed_W = i * 0.1\n","    curr_cost, curr_W = sess.run([cost,W], feed_dict={W:feed_W})\n","    W_val.append(curr_W)\n","    cost_val.append(curr_cost)\n","\n","plt.plot(W_val, cost_val)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JjtzOkGYHC3c","colab_type":"code","colab":{}},"source":["# Lab 3 Minimizing Cost\n","import tensorflow as tf\n","\n","tf.set_random_seed(777)  # for reproducibility\n","\n","x_data = [1, 2, 3]\n","y_data = [1, 2, 3]\n","\n","# Try to find values for W and b to compute y_data = W * x_data\n","# We know that W should be 1\n","# But let's use TensorFlow to figure it out\n","W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n","\n","X = tf.placeholder(tf.float32)\n","Y = tf.placeholder(tf.float32)\n","\n","# Our hypothesis for linear model X * W\n","hypothesis = X * W\n","\n","# cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n","learning_rate = 0.1\n","gradient = tf.reduce_mean((W * X - Y) * X)\n","descent = W - learning_rate * gradient\n","update = W.assign(descent)\n","\n","# Launch the graph in a session.\n","with tf.Session() as sess:\n","    # Initializes global variables in the graph.\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(21):\n","        _, cost_val, W_val = sess.run(\n","            [update, cost, W], feed_dict={X: x_data, Y: y_data}\n","        )\n","        print(step, cost_val, W_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2tcfnaYIXns","colab_type":"code","colab":{}},"source":["\n","# Lab 4 Multi-variable linear regression\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility\n","\n","x1_data = [73., 93., 89., 96., 73.]\n","x2_data = [80., 88., 91., 98., 66.]\n","x3_data = [75., 93., 90., 100., 70.]\n","\n","y_data = [152., 185., 180., 196., 142.]\n","\n","# placeholders for a tensor that will be always fed.\n","x1 = tf.placeholder(tf.float32)\n","x2 = tf.placeholder(tf.float32)\n","x3 = tf.placeholder(tf.float32)\n","\n","Y = tf.placeholder(tf.float32)\n","\n","w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n","w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n","w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n","\n","# cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","# Minimize. Need a very small learning rate for this data set\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n","train = optimizer.minimize(cost)\n","\n","# Launch the graph in a session.\n","sess = tf.Session()\n","# Initializes global variables in the graph.\n","sess.run(tf.global_variables_initializer())\n","\n","for step in range(2001):\n","    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n","                                   feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n","    if step % 10 == 0:\n","        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PF-4Ihr7ar27","colab_type":"code","colab":{}},"source":["# Lab 4 Multi-variable linear regression\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility\n","\n","x_data = [[73., 80., 75.],\n","          [93., 88., 93.],\n","          [89., 91., 90.],\n","          [96., 98., 100.],\n","          [73., 66., 70.]]\n","y_data = [[152.],\n","          [185.],\n","          [180.],\n","          [196.],\n","          [142.]]\n","\n","\n","# placeholders for a tensor that will be always fed.\n","X = tf.placeholder(tf.float32, shape=[None, 3]) #값을 3개받아서 3의 값이 들어간다.\n","Y = tf.placeholder(tf.float32, shape=[None, 1])\n","\n","W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","# Hypothesis\n","hypothesis = tf.matmul(X, W) + b\n","\n","# Simplified cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","# Minimize\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n","train = optimizer.minimize(cost)\n","\n","# Launch the graph in a session.\n","sess = tf.Session()\n","# Initializes global variables in the graph.\n","sess.run(tf.global_variables_initializer())\n","\n","for step in range(2001):\n","    cost_val, hy_val, _ = sess.run(\n","        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n","    if step % 10 == 0:\n","        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OimSWXE3bByp","colab_type":"code","colab":{}},"source":["# Lab 4 Multi-variable linear regression\n","import tensorflow as tf\n","import numpy as np\n","tf.set_random_seed(777)  # for reproducibility\n","\n","xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","\n","# Make sure the shape and data are OK\n","print(x_data, \"\\nx_data shape:\", x_data.shape)\n","print(y_data, \"\\ny_data shape:\", y_data.shape)\n","\n","# data output\n","'''\n","[[ 73.  80.  75.]\n"," [ 93.  88.  93.]\n"," ...\n"," [ 76.  83.  71.]\n"," [ 96.  93.  95.]] \n","x_data shape: (25, 3)\n","[[152.]\n"," [185.]\n"," ...\n"," [149.]\n"," [192.]] \n","y_data shape: (25, 1)\n","'''\n","\n","# placeholders for a tensor that will be always fed.\n","X = tf.placeholder(tf.float32, shape=[None, 3])\n","Y = tf.placeholder(tf.float32, shape=[None, 1])\n","\n","W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","# Hypothesis\n","hypothesis = tf.matmul(X, W) + b\n","\n","# Simplified cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","# Minimize\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n","train = optimizer.minimize(cost)\n","\n","# Launch the graph in a session.\n","sess = tf.Session()\n","# Initializes global variables in the graph.\n","sess.run(tf.global_variables_initializer())\n","\n","for step in range(2001):\n","    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], \n","                                   feed_dict={X: x_data, Y: y_data})\n","    if step % 10 == 0:\n","        print(step, \"Cost:\", cost_val, \"\\nPrediction:\\n\", hy_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HB8Mfc7CeA7u","colab_type":"text"},"source":["# csv  파일 가져오기 "]},{"cell_type":"code","metadata":{"id":"APJzTDD_dTzu","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","#pandas read_csv로 불러오기\n","import io\n","import pandas as pd\n","\n","drive.mount('/content/drive')\n","filename = '/content/drive/My Drive/test.csv'\n","filename\n","data = pd.read_csv(filename)\n","data.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6RUIUiRdoDJ","colab_type":"code","colab":{}},"source":["filename = '/content/drive/My Drive/test.csv'\n","filename"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeAAQABDkZmz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6qQ9t-DdwbU","colab_type":"code","colab":{}},"source":["#pandas read_csv로 불러오기\n","import io\n","import pandas as pd\n","\n","data = pd.read_csv(filename)\n","data.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Pei_11MdzX8","colab_type":"code","colab":{}},"source":["data[3:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cyemb82Kkaro","colab_type":"code","colab":{}},"source":["# Lab 4 Multi-variable linear regression\n","# https://www.tensorflow.org/programmers_guide/reading_data\n","\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility\n","\n","filename_queue = tf.train.string_input_producer(\n","    ['data-01-test-score.csv'], shuffle=False, name='filename_queue')\n","\n","reader = tf.TextLineReader()\n","key, value = reader.read(filename_queue)\n","\n","# Default values, in case of empty columns. Also specifies the type of the\n","# decoded result.\n","record_defaults = [[0.], [0.], [0.], [0.]]\n","xy = tf.decode_csv(value, record_defaults=record_defaults)\n","\n","# collect batches of csv in\n","train_x_batch, train_y_batch = \\\n","    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n","\n","# placeholders for a tensor that will be always fed.\n","X = tf.placeholder(tf.float32, shape=[None, 3])\n","Y = tf.placeholder(tf.float32, shape=[None, 1])\n","\n","W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","# Hypothesis\n","hypothesis = tf.matmul(X, W) + b\n","\n","# Simplified cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","# Minimize\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n","train = optimizer.minimize(cost)\n","\n","# Launch the graph in a session.\n","sess = tf.Session()\n","# Initializes global variables in the graph.\n","sess.run(tf.global_variables_initializer())\n","\n","# Start populating the filename queue.\n","coord = tf.train.Coordinator()\n","threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n","\n","for step in range(2001):\n","    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n","    cost_val, hy_val, _ = sess.run(\n","        [cost, hypothesis, train], feed_dict={X: x_batch, Y: y_batch})\n","    if step % 10 == 0:\n","        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n","\n","coord.request_stop()\n","coord.join(threads)\n","\n","# Ask my score\n","print(\"Your score will be \",\n","      sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))\n","\n","print(\"Other scores will be \",\n","      sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpQ2sB6JzVbV","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YsHmEdeuzrkD","colab_type":"text"},"source":["# classification\n"]},{"cell_type":"code","metadata":{"id":"667TAKEZ4imp","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","#pandas read_csv로 불러오기\n","import io\n","import pandas as pd\n","\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYZCdu0Zzuap","colab_type":"code","colab":{}},"source":["# Lab 5 Logistic Regression Classifier\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility\n","\n","x_data = [[1, 2],\n","          [2, 3],\n","          [3, 1],\n","          [4, 3],\n","          [5, 3],\n","          [6, 2]]\n","y_data = [[0],\n","          [0],\n","          [0],\n","          [1],\n","          [1],\n","          [1]]\n","\n","# placeholders for a tensor that will be always fed.\n","X = tf.placeholder(tf.float32, shape=[None, 2])\n","Y = tf.placeholder(tf.float32, shape=[None, 1])\n","\n","W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n","hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n","\n","# cost/loss function\n","cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n","                       tf.log(1 - hypothesis))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n","\n","# Accuracy computation\n","# True if hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(10001):\n","        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n","        if step % 200 == 0:\n","            print(step, cost_val)\n","\n","    # Accuracy report\n","    h, c, a = sess.run([hypothesis, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o7aOy3-l5D9p","colab_type":"code","colab":{}},"source":["filename = '/content/drive/My Drive/data-03-diabetes.csv'\n","filename\n","data = pd.read_csv(filename)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFez-SJB3Xmc","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jX2dC0Jv4QRt","colab_type":"code","colab":{}},"source":["# Lab 5 Logistic Regression Classifier\n","import tensorflow as tf\n","import numpy as np\n","tf.set_random_seed(777)  # for reproducibility\n","\n","xy = np.loadtxt(filename, delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","\n","print(x_data.shape, y_data.shape)\n","\n","# placeholders for a tensor that will be always fed.\n","X = tf.placeholder(tf.float32, shape=[None, 8])\n","Y = tf.placeholder(tf.float32, shape=[None, 1])\n","\n","W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\n","hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n","\n","# cost/loss function\n","cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n","                       tf.log(1 - hypothesis))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n","\n","# Accuracy computation\n","# True if hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(10001):\n","        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n","        if step % 200 == 0:\n","            print(step, cost_val)\n","\n","    # Accuracy report\n","    h, c, a = sess.run([hypothesis, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"],"execution_count":null,"outputs":[]}]}