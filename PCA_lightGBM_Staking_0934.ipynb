{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA_lightGBM_Staking.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkWj-MkrckaF"
      },
      "source": [
        "# matplotlib 한글 깨짐 해결\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "# 이 셀 실행 후 런터임 재실행을 실시하세요"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQswIGetcots"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import random\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') # 한글설정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZHiL1d8cor7"
      },
      "source": [
        "# 파이썬 지수를 정수로 표현!\n",
        "pd.options.display.float_format = '{:.5f}'.format\n",
        "\n",
        "# 코랩 그래프 크게!\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (40, 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43lJnd_cop_"
      },
      "source": [
        "\n",
        "# 한글설정\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "\n",
        "# 파이썬 지수를 정수로 표현\n",
        "pd.options.display.float_format = '{:.5f}'.format\n",
        "\n",
        "# 구글 드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 데이터 불러오기\n",
        "\n",
        "test = pd.read_csv('/content/drive/My Drive/data/bigcorn_data/2019test_(3).csv', encoding = 'cp949')\n",
        "train = pd.read_csv('/content/drive/My Drive/data/bigcorn_data/2019train_(3).csv', encoding = 'cp949')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEAJWEbeRG36"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOLiYjsAconD"
      },
      "source": [
        "\n",
        "print('test', test.columns)\n",
        "print('train', train.columns)\n",
        "\n",
        "print(test.shape)\n",
        "print(train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTz8UodRnxIb"
      },
      "source": [
        "def get_stacking_data(model, X_train, y_train, X_test):\n",
        "    from sklearn.model_selection import KFold\n",
        "    # 지정된 n_folds값으로 kfold 생성\n",
        "    kfold = KFold(n_splits=3,random_state=0)\n",
        "\n",
        "    # 추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화\n",
        "    train_fold_predict = np.zeros((X_train.shape[0],1))\n",
        "    test_predict = np.zeros((X_test.shape[0], 3))\n",
        "    print(type(test_predict))\n",
        "    print(type(test_predict))\n",
        "\n",
        "    print(\"model:\", model.__class__.__name__)\n",
        "\n",
        "    for cnt, (train_index, valid_index) in enumerate(kfold.split(X_train)) :\n",
        "\n",
        "        X_tr  = X_train.iloc[train_index]\n",
        "        y_tr = y_train.iloc[train_index]\n",
        "        X_te = X_train.iloc[valid_index]\n",
        "        model.fit( X_tr, y_tr)\n",
        "    # # 폴더 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장\n",
        "        train_fold_predict[valid_index, :]=model.predict(X_te).reshape(-1,1)\n",
        "\n",
        "    # # 입력된 원본 데스트 데이터를 폴드 세트 내 학습된 기반 모델에서 예측 후 데이터 저장\n",
        "        test_predict[:, cnt] = model.predict(X_test).reshape(1,-1)\n",
        "\n",
        "    test_predict_mean = np.mean(test_predict, axis=1).reshape(-1,1)\n",
        "\n",
        "  # train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean 테스트 데이터\n",
        "    return train_fold_predict, test_predict_mean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wON2B4Bn0A5"
      },
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEJj3RSvcogH"
      },
      "source": [
        "# MAPE 구해주는 함수\n",
        "def func_mape(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "## 취급액boxcox > 취급액변환\n",
        "def original_target(boxcox):\n",
        "    return (0.2222222 * boxcox +1)**(1/0.2222222)\n",
        "\n",
        "# 각각의 Valdation set마다 PCA를 적용해 주고, cross validation까지 수행해주는 함수\n",
        "def PCA_CV(K, train, params, all_feature, pca_feature, target, onehot_feature):\n",
        "    all_mape = []\n",
        "    cnt = 1\n",
        "    np.random.seed(1234)\n",
        "    kfold = KFold(n_splits=K, shuffle=False)\n",
        "    for t, v in kfold.split(train):\n",
        "        # train / val > index 부여\n",
        "        train_cv = train.iloc[t]\n",
        "        val_cv = train.iloc[v]\n",
        "\n",
        "        # train : x, y 나누기\n",
        "        x_train = train_cv.loc[:, all_feature]\n",
        "        y_train = train_cv.loc[:, target]\n",
        "        y_train = y_train.iloc[:, 0]\n",
        "\n",
        "        # val : x, y 나누기\n",
        "        x_val = val_cv.loc[:, all_feature]\n",
        "        y_val = val_cv.loc[:, target]\n",
        "        y_val = y_val.iloc[:, 0]\n",
        "        \n",
        "\n",
        "        if len(pca_feature) != 0 :\n",
        "\n",
        "            # PCA를 진행할 칼럼만 추출\n",
        "            x_train_PCA = x_train.loc[:, pca_feature]\n",
        "            remain_x_train = x_train.drop(pca_feature, axis=1, inplace = False).reset_index(drop = True)\n",
        "        \n",
        "            x_val_PCA = x_val.loc[:, pca_feature]\n",
        "            remain_x_val = x_val.drop(pca_feature, axis=1, inplace = False).reset_index(drop = True)\n",
        "            \n",
        "            # PCA를 위한 Z-정규화\n",
        "            m = np.mean(x_train_PCA)\n",
        "            s = np.std(x_train_PCA)\n",
        "            standard_x_train_PCA = (x_train_PCA - m) / s\n",
        "            standard_x_val_PCA = (x_val_PCA - m) / s\n",
        "\n",
        "            \n",
        "            # PCA진행\n",
        "            # 1. train_set\n",
        "            pca1 = PCA(n_components=5)    ########################################### 임의로 PCA의 수를 지정해 줄 수 있어요\n",
        "            pca_x_train = pca1.fit_transform(standard_x_train_PCA)\n",
        "            pca_x_train_df = pd.DataFrame(data = pca_x_train\n",
        "                    , columns = ['pca1', 'pca2', 'pca3'])\n",
        "            # 2. val_set\n",
        "            pca2 = PCA(n_components=5)    ########################################### 임의로 PCA의 수를 지정해 줄 수 있어요 (위에랑 같은 숫자로 지정해 주어야 해요 ㅠ)\n",
        "            pca_x_val = pca2.fit_transform(standard_x_val_PCA)\n",
        "            pca_x_val_df = pd.DataFrame(data = pca_x_val\n",
        "                    , columns = ['pca1', 'pca2', 'pca3'])\n",
        "            \n",
        "\n",
        "            PCA_x_train = pd.concat([remain_x_train, pca_x_train_df],  axis = 1, sort=False)\n",
        "            PCA_x_val = pd.concat([remain_x_val, pca_x_val_df],  axis = 1, sort=False)\n",
        "\n",
        "            # OneHot Encoding\n",
        "            all_data  = PCA_x_train.append(PCA_x_val)\n",
        "            onehot_all_data = pd.get_dummies(data = all_data, columns=onehot_feature, drop_first = False)\n",
        "\n",
        "            x_train = onehot_all_data.iloc[:len(PCA_x_train), ]\n",
        "            x_val = onehot_all_data.iloc[len(PCA_x_train):, ]\n",
        "\n",
        "\n",
        "        else :\n",
        "            all_data = x_train.append(x_val)\n",
        "            onehot_all_data = pd.get_dummies(data = all_data, columns=onehot_feature, drop_first = False)\n",
        "\n",
        "            x_train = onehot_all_data.iloc[0:len(x_train), ]\n",
        "            x_val = onehot_all_data.iloc[len(x_train):len(onehot_all_data), ]\n",
        "\n",
        "        ############################################################# 변수 전처리 완료!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "        # 모델\n",
        "        # model = lgb.LGBMRegressor(learning_rate = params['learning_rate'],\n",
        "        #                           num_iterations = params['num_iterations'],\n",
        "        #                           max_depth = params['max_depth'],\n",
        "        #                           boosting  = 'dart',\n",
        "        #                           eval_metrics = 'mape')\n",
        "        # model.fit(x_train, y_train)\n",
        "\n",
        "        # y_pred = model.predict(x_val)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # 모델링넣는부분 \n",
        "        lgbm_reg = lgb.LGBMRegressor(learning_rate = 0.1,\n",
        "                              num_iterations = 1000,\n",
        "                              max_depth = 6,\n",
        "                              feature_fraction = 0.7,\n",
        "                              boosting  = 'dart',\n",
        "                              eval_metrics = 'mape')\n",
        "\n",
        "        xgb_reg = XGBRegressor(learning_rate=0.03,max_depth_list=6,min_child_weight=1,gamma=0.0)\n",
        "        hgb = HistGradientBoostingRegressor(max_leaf_nodes=31,learning_rate=0.05,min_samples_leaf=26,max_iter=120)\n",
        "\n",
        "\n",
        "        #모델 추가 :\n",
        "\n",
        "        # 모델_train , 모델_test = get_stacking_data(모델, x_train, y_train,x_val) \n",
        "        hgb_train, hgb_test = get_stacking_data(hgb, x_train, y_train,x_val)\n",
        "        xgb_reg_train, xgb_reg_test = get_stacking_data(xgb_reg, x_train, y_train,x_val)\n",
        "        lgbm_reg_train, lgbm_reg_test = get_stacking_data(lgbm_reg, x_train, y_train,x_val)\n",
        "\n",
        "        #                   = np.concatenate(( hgb_train, xgb_reg_train, lgbm_reg_train , 딥러닝_Train), axis=1)\n",
        "        Stack_final_X_train = np.concatenate(( hgb_train, xgb_reg_train, lgbm_reg_train), axis=1)\n",
        "\n",
        "        #                   = np.concatenate(( hgb_train, xgb_reg_train, lgbm_reg_train , 딥러닝_test), axis=1)\n",
        "        Stack_final_X_test = np.concatenate((hgb_test, xgb_reg_test, lgbm_reg_test), axis=1)\n",
        "        lgbm_reg_train.fit(Stack_final_X_train, y_train)\n",
        "        final = lgbm_reg_train.predict(Stack_final_X_test)\n",
        "\n",
        "        # model =  RandomForestRegressor(random_state=0)\n",
        "        # model.fit(x_train, y_train)\n",
        "        # y_pred = model.predict(x_val)\n",
        "\n",
        "\n",
        "        if target[0] == '취급액' :\n",
        "            mape = func_mape(y_val, final)\n",
        "            print(cnt, '-Fold MAPE : ' ,mape)\n",
        "            all_mape.append(mape)\n",
        "            cnt += 1\n",
        "        else:\n",
        "            mape = func_mape(original_target(y_val), original_target(final))\n",
        "            print(cnt, '-Fold MAPE : ' ,mape)\n",
        "            all_mape.append(mape)\n",
        "            cnt += 1\n",
        "    \n",
        "\n",
        "    print('---------------------------------')\n",
        "    print('3-Fold Mean MAPE : ', np.mean(all_mape))\n",
        "    print('모든 mape : ', all_mape)\n",
        "    print(' ')\n",
        "\n",
        "    # 변수중요도 플랏\n",
        "    # df = {'feature' : pd.Series(x_train.columns), 'importance' : pd.Series(model.feature_importances_)}\n",
        "    # importance_df = pd.DataFrame(df)\n",
        "    # importance_df = importance_df.sort_values(by = ['importance'])\n",
        "    # importance_df.plot.barh(y = 'importance', x = 'feature')\n",
        "\n",
        "    return final, y_val, np.mean(all_mape),xgb_reg, Stack_final_X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkGisF6xm9WA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8pKT8MCY-RO"
      },
      "source": [
        "# 실행해서 우리 데이터에 어떤 칼럼이 존재하는지 보세요\n",
        "# 'Unnamed: 0'은 index번호라서 무조건 뺴면되요~\n",
        "\n",
        "print('train', train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf9u3J_XnGkG"
      },
      "source": [
        "# K-Fold 지정\n",
        "K = 3\n",
        "\n",
        "# 사용하고자 하는 *모든* 변수를 입력하세요\n",
        "all_feature = ['노출(분)',\n",
        "'판매단가',\n",
        "'월',\n",
        "'일',\n",
        "'168시간',\n",
        "'매진여부',\n",
        "'count',\n",
        "'한달_상품군빈도',\n",
        "'한달_중분류빈도',\n",
        "'한달_소분류빈도',\n",
        "'한달_그룹코드빈도',\n",
        "'기온',\n",
        "'강수량',\n",
        "'풍속',\n",
        "'습도',\n",
        "'적설',\n",
        "'전운량',\n",
        "'미세먼지',\n",
        "'초미세먼지',\n",
        "'판매단가순위',\n",
        "'상품군판매단가순위',\n",
        "'상품군요일순위',\n",
        "'상품군24시간순위',\n",
        "'중분류요일순위',\n",
        "'중분류24시간순위',\n",
        "'상품군168시간순위',\n",
        "'중분류168시간순위',\n",
        "'판매단가rank',\n",
        "'시간순위',\n",
        "'시간순위168',\n",
        "'브랜드순위',\n",
        "'중분류순위',\n",
        "'소분류순위',\n",
        "'방송내_순서',\n",
        "'방송순서',\n",
        "'판매상품종류수',\n",
        "'판매상품종류비율',\n",
        "'그룹코드_전체횟수',\n",
        "'그룹코드_쪽박횟수',\n",
        "'그룹코드_대박횟수',\n",
        "'그룹코드_쪽박확률',\n",
        "'그룹코드_대박확률',\n",
        "'브랜드별_전체횟수',\n",
        "'브랜드별_대박횟수',\n",
        "'브랜드별_대박확률',\n",
        "'중분류별_전체횟수',\n",
        "'중분류별_대박횟수',\n",
        "'중분류별_대박확률',\n",
        "'소분류별_전체횟수',\n",
        "'소분류별_대박횟수',\n",
        "'소분류별_대박확률',\n",
        "'168시간_COS',\n",
        "'168시간_SIN',\n",
        "'24시간_COS',\n",
        "'24시간_SIN',\n",
        "'방송내_상품종류별_점수',\n",
        "'전체편성비율',\n",
        "'방송내_상품선호도',\n",
        "'결제수단']\n",
        "\n",
        "# PCA를 수행하고자 하는 변수를 입력하세요\n",
        "pca_feature = []\n",
        "\n",
        "# Target 변수를 입력하세요 '취급액'으로 넣어도 됩니다.\n",
        "target = ['취급액boxcox']\n",
        "\n",
        "# all_feature에 입력했던 범주형 변수를 입력하세요\n",
        "onehot_feature = ['결제수단']\n",
        "# 파라매터를 입력하세요\n",
        "params = {'learning_rate' : 0.1, 'num_iterations' : 1000, 'max_depth' : 6}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2ZwIqfThOYv"
      },
      "source": [
        "# Grid search 없이 cross validation만 수행\n",
        "\n",
        "y_pred, y_real, temp,model,X = PCA_CV(K, train, params, all_feature, pca_feature, target, onehot_feature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgHPYpzt6CYZ"
      },
      "source": [
        "print(y_pred)\n",
        "print(y_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3E_PlCb6cke"
      },
      "source": [
        "# Grid search 없이 cross validation만 수행\n",
        "\n",
        "y_pred, y_real, temp,model,X = PCA_CV(K, train, params, all_feature, pca_feature, target, onehot_feature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQkBgipJFn4y"
      },
      "source": [
        "pd.Series(list(X))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAy3RTnI6dma"
      },
      "source": [
        "imp = model.feature_importances_\n",
        "feature_list = pd.concat([pd.Series(list(X)), pd.Series(model.feature_importances_)], axis=1)\n",
        "feature_list.columns = ['features_name', 'importance']\n",
        "feature_list.sort_values(\"importance\", ascending =False)[:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX8ek5EGuL4U"
      },
      "source": [
        "imp = model.feature_importances_\n",
        "feature_list = pd.concat([pd.Series(list(X)), pd.Series(model.feature_importances_)], axis=1)\n",
        "feature_list.columns = ['features_name', 'importance']\n",
        "feature_list.sort_values(\"importance\", ascending =True)[:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDsxD87RgrWG"
      },
      "source": [
        "# Grid search 없이 cross validation만 수행\n",
        "\n",
        "y_pred, y_real, temp = PCA_CV(K, train, params, all_feature, pca_feature, target, onehot_feature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYqAOFmiY8cA"
      },
      "source": [
        "# Grid search 없이 cross validation만 수행\n",
        "\n",
        "y_pred, y_real, temp = PCA_CV(K, train, params, all_feature, pca_feature, target, onehot_feature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VPQqXuOV9_c"
      },
      "source": [
        "a = original_target(y_real)\n",
        "b = original_target(y_pred)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h87OTCl3d-KU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4kY5f8WYlze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNGsLgEl0iGd"
      },
      "source": [
        "# 실제 Target과 예측 Target의 그래프 추이\n",
        "plt.rcParams['figure.figsize'] = (20, 5)\n",
        "\n",
        "data = {\n",
        "    'real' : y_real,\n",
        "    'pred' : y_pred\n",
        "}\n",
        "\n",
        "\n",
        "data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.plot.line(linewidth = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgTPW4mmVwob"
      },
      "source": [
        "# 사용하고자 하는 *모든* 변수를 입력하세요\n",
        "all_feature = ['노출(분)',\n",
        "'판매단가',\n",
        "'월',\n",
        "'일',\n",
        "'168시간',\n",
        "'매진여부',\n",
        "'count',\n",
        "'한달_상품군빈도',\n",
        "'한달_중분류빈도',\n",
        "'한달_소분류빈도',\n",
        "'한달_그룹코드빈도',\n",
        "'기온',\n",
        "'강수량',\n",
        "'풍속',\n",
        "'습도',\n",
        "'적설',\n",
        "'전운량',\n",
        "'미세먼지',\n",
        "'초미세먼지',\n",
        "'판매단가순위',\n",
        "'상품군판매단가순위',\n",
        "'상품군요일순위',\n",
        "'상품군24시간순위',\n",
        "'중분류요일순위',\n",
        "'중분류24시간순위',\n",
        "'상품군168시간순위',\n",
        "'중분류168시간순위',\n",
        "'판매단가rank',\n",
        "'시간순위',\n",
        "'시간순위168',\n",
        "'브랜드순위',\n",
        "'중분류순위',\n",
        "'소분류순위',\n",
        "'방송내_순서',\n",
        "'방송순서',\n",
        "'판매상품종류수',\n",
        "'판매상품종류비율',\n",
        "'그룹코드_전체횟수',\n",
        "'그룹코드_쪽박횟수',\n",
        "'그룹코드_대박횟수',\n",
        "'그룹코드_쪽박확률',\n",
        "'그룹코드_대박확률',\n",
        "'브랜드별_전체횟수',\n",
        "'브랜드별_대박횟수',\n",
        "'브랜드별_대박확률',\n",
        "'중분류별_전체횟수',\n",
        "'중분류별_대박횟수',\n",
        "'중분류별_대박확률',\n",
        "'소분류별_전체횟수',\n",
        "'소분류별_대박횟수',\n",
        "'소분류별_대박확률',\n",
        "'168시간_COS',\n",
        "'168시간_SIN',\n",
        "'24시간_COS',\n",
        "'24시간_SIN',\n",
        "'방송내_상품종류별_점수',\n",
        "'최소판매수량',\n",
        "'전체편성비율',\n",
        "'방송내_상품선호도',\n",
        "'결제수단']\n",
        "\n",
        "# PCA를 수행하고자 하는 변수를 입력하세요\n",
        "pca_feature = []\n",
        "\n",
        "# Target 변수를 입력하세요 '취급액'으로 넣어도 됩니다.\n",
        "target = ['취급액boxcox']\n",
        "\n",
        "# all_feature에 입력했던 범주형 변수를 입력하세요\n",
        "onehot_feature = ['결제수단']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icj6FL6OcoaP"
      },
      "source": [
        "# Gridsearch 구현\n",
        "\n",
        "K = 3\n",
        "# all_feature = []  # 사용하는 변수를 다 집어 넣으세요\n",
        "# pca_feature = []  # PCA를 진행하고자 하는 변수를 집어 넣으세요\n",
        "# target = ['취급액boxcox'] # 타겟변수를 집어 넣으세요 / '취급액' or '취급액boxcox' \n",
        "# onehot_feature = [] # 범주형 변수를 집어 넣으세요\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "min = 99999\n",
        "for learning_rate in [0.1, 0.2, 0.3]:  # learning_rate\n",
        "    for num_iterations in [1000, 1200]: #num_iterations\n",
        "        for max_depth in [6, 8] : # max_depth\n",
        "            for feature_fraction in [0.7, 0.8]:\n",
        "                params = {'learning_rate' : learning_rate, 'num_iterations' : num_iterations, 'max_depth' : max_depth, 'feature_fraction' : feature_fraction}\n",
        "                y_pred, y_real, temp = PCA_CV(K, train, params, all_feature, pca_feature, target, onehot_feature)\n",
        "                if min > temp :\n",
        "                    min = temp\n",
        "                    best_param = params\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print('가장 낮은 MAPE : ', min) # 가장 낮은 MAPE를 출력합니다\n",
        "print('Best parametor :', best_param) # MAPE가 가장 낮을때의 파라매터를 출력합니다\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wDYDDwRcoXe"
      },
      "source": [
        "print(min)\n",
        "print(best_param)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9euDVDHcoVd"
      },
      "source": [
        "kfold = KFold(n_splits=3, shuffle=False)\n",
        "\n",
        "for a, b in kfold.split(train):\n",
        "    print('train : ', a)\n",
        "    print('test :', b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1PPipEoky7j"
      },
      "source": [
        "### 최종예측!\n",
        "\n",
        "\n",
        "# MAPE 구해주는 함수\n",
        "def func_mape(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "### Final Predict\n",
        "def final_predict(train, test, params, all_feature, pca_feature, target, onehot_feature):\n",
        "    # train : x, y 나누기\n",
        "    np.random.seed(1234)\n",
        "    x_train = train.loc[:, all_feature]\n",
        "    y_train = train.loc[:, target]\n",
        "    y_train = y_train.iloc[:, 0]\n",
        "\n",
        "    # test : x, y 나누기\n",
        "    x_test = test.loc[:, all_feature]\n",
        "    y_test = test.loc[:, target]\n",
        "    y_test = y_test.iloc[:, 0]\n",
        "\n",
        "    if len(pca_feature) != 0 :\n",
        "        # PCA를 진행할 칼럼만 추출\n",
        "        x_train_PCA = x_train.loc[:, pca_feature]\n",
        "        remain_x_train = x_train.drop(pca_feature, axis=1, inplace = False).reset_index(drop = True)\n",
        "        \n",
        "        x_val_PCA = x_test.loc[:, pca_feature]\n",
        "        remain_x_test = x_test.drop(pca_feature, axis=1, inplace = False).reset_index(drop = True)\n",
        "            \n",
        "            # PCA를 위한 Z-정규화\n",
        "        m = np.mean(x_train_PCA)\n",
        "        s = np.std(x_train_PCA)\n",
        "        standard_x_train_PCA = (x_train_PCA - m) / s\n",
        "        standard_x_test_PCA = (x_test_PCA - m) / s\n",
        "\n",
        "            \n",
        "        # PCA진행\n",
        "        # 1. train_set\n",
        "        pca1 = PCA(n_components=5)    ########################################### 임의로 PCA의 수를 지정해 줄 수 있어요\n",
        "        pca_x_train = pca1.fit_transform(standard_x_train_PCA)\n",
        "        pca_x_train_df = pd.DataFrame(data = pca_x_train\n",
        "                , columns = ['pca1', 'pca2', 'pca3', 'pca4', 'pca5'])\n",
        "        # 2. val_set\n",
        "        pca2 = PCA(n_components=5)    ########################################### 임의로 PCA의 수를 지정해 줄 수 있어요 (위에랑 같은 숫자로 지정해 주어야 해요 ㅠ)\n",
        "        pca_x_test = pca2.fit_transform(standard_x_test_PCA)\n",
        "        pca_x_test_df = pd.DataFrame(data = pca_x_val\n",
        "                , columns = ['pca1', 'pca2', 'pca3', 'pca4', 'pca5'])\n",
        "            \n",
        "\n",
        "        PCA_x_train = pd.concat([remain_x_train, pca_x_train_df],  axis = 1, sort=False)\n",
        "        PCA_x_val = pd.concat([remain_x_val, pca_x_val_df],  axis = 1, sort=False)\n",
        "\n",
        "        # OneHot Encoding\n",
        "        all_data  = PCA_x_train.append(PCA_x_test)\n",
        "        onehot_all_data = pd.get_dummies(data = all_data, columns=onehot_feature, drop_first = False)\n",
        "\n",
        "        x_train = onehot_all_data.iloc[:len(PCA_x_train), ]\n",
        "        x_test = onehot_all_data.iloc[len(PCA_x_train):, ]        \n",
        "\n",
        "    else:\n",
        "        all_data = x_train.append(x_test)\n",
        "        onehot_all_data = pd.get_dummies(data = all_data, columns=onehot_feature, drop_first = False)\n",
        "\n",
        "        x_train = onehot_all_data.iloc[0:len(x_train), ]\n",
        "        x_test = onehot_all_data.iloc[len(x_train):len(onehot_all_data), ]        \n",
        "\n",
        "    ## 예측시작\n",
        "    np.random.seed(1234)\n",
        "    # model = lgb.LGBMRegressor(learning_rate = params['learning_rate'],\n",
        "    #                           num_iterations = params['num_iterations'],\n",
        "    #                           max_depth = params['max_depth'],\n",
        "    #                           boosting  = 'dart',\n",
        "    #                           eval_metrics = 'mape')\n",
        "   \n",
        "\n",
        "    lgbm_reg = lgb.LGBMRegressor(learning_rate = 0.1,\n",
        "                              num_iterations = 1000,\n",
        "                              max_depth = 6,\n",
        "                              feature_fraction = 0.7,\n",
        "                              boosting  = 'dart',\n",
        "                              eval_metrics = 'mape')\n",
        "\n",
        "    xgb_reg = XGBRegressor(learning_rate=0.03,max_depth_list=6,min_child_weight=1,gamma=0.0)\n",
        "    hgb = HistGradientBoostingRegressor(max_leaf_nodes=31,learning_rate=0.05,min_samples_leaf=26,max_iter=120)\n",
        "\n",
        "    hgb_train, hgb_test = get_stacking_data(hgb, x_train, y_train,x_test)\n",
        "    xgb_reg_train, xgb_reg_test = get_stacking_data(xgb_reg, x_train, y_train,x_test)\n",
        "    lgbm_reg_train, lgbm_reg_test = get_stacking_data(lgbm_reg, x_train, y_train,x_test)\n",
        "\n",
        "    Stack_final_X_train = np.concatenate(( hgb_train, xgb_reg_train, lgbm_reg_train), axis=1)\n",
        "    Stack_final_X_test = np.concatenate((hgb_test, xgb_reg_test, lgbm_reg_test), axis=1)\n",
        "    xgb_reg.fit(Stack_final_X_train, y_train)\n",
        "    final = xgb_reg.predict(Stack_final_X_test)\n",
        "\n",
        "\n",
        "\n",
        "    if target[0] == '취급액':\n",
        "        mape = func_mape(y_test, final)\n",
        "        print('Final MAPE : ' ,mape)\n",
        "        y_pred = y_pred\n",
        "        y_test = np.array(y_test)\n",
        "        mape = mape\n",
        "\n",
        "    else:\n",
        "        final = original_target(final)\n",
        "        y_test = original_target(np.array(y_test))\n",
        "        mape = func_mape(y_test, final)\n",
        "        print('Final MAPE : ' ,mape)\n",
        "    \n",
        "    # 변수중요도 플랏\n",
        "    df = {'feature' : pd.Series(x_train.columns), 'importance' : pd.Series(model.feature_importances_)}\n",
        "    importance_df = pd.DataFrame(df)\n",
        "    importance_df = importance_df.sort_values(by = ['importance'])\n",
        "    importance_df.plot.barh(y = 'importance', x = 'feature')\n",
        "\n",
        "    return y_pred, y_test, mape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5GQ0m-xp4cE"
      },
      "source": [
        "# # K-Fold 지정\n",
        "# K = 3\n",
        "\n",
        "# # 사용하고자 하는 *모든* 변수를 입력하세요\n",
        "# all_feature = []\n",
        "\n",
        "# # PCA를 수행하고자 하는 변수를 입력하세요\n",
        "# pca_feature = []\n",
        "\n",
        "# # Target 변수를 입력하세요 '취급액'으로 넣어도 됩니다.\n",
        "# target = []\n",
        "\n",
        "# # all_feature에 입력했던 범주형 변수를 입력하세요\n",
        "# onehot_feature = []\n",
        "\n",
        "\n",
        "# params = {'learning_rate' : 0.1, 'num_iterations' : 1000, 'max_depth' : 6}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0YFLnLBkyzg"
      },
      "source": [
        "# test data : 2019.06 예측\n",
        "\n",
        "y_pred, y_real, mape = final_predict(train, test, params, all_feature, pca_feature, target, onehot_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrfB6oeLm_sv"
      },
      "source": [
        "print(y_pred)\n",
        "print(y_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XykESVW6xyR8"
      },
      "source": [
        "print(y_pred)\n",
        "print(y_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxIAK_Ikm_nx"
      },
      "source": [
        "# 실제 Target과 예측 Target의 그래프 추이\n",
        "plt.rcParams['figure.figsize'] = (200, 40)\n",
        "\n",
        "data = {\n",
        "    'real' : y_real,\n",
        "    'pred' : y_pred\n",
        "}\n",
        "\n",
        "\n",
        "data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.plot.line(linewidth = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcBjjXev-GOJ"
      },
      "source": [
        "data = {\n",
        "    'real' : y_real,\n",
        "    'pred' : y_pred,\n",
        "    'MAPE' : abs( ( y_real - y_pred ) / y_real ) * 100\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "newdf = pd.concat([test.loc[:, ['방송일시', '상품명', '상품군']], df], axis = 1, sort=False)\n",
        "n_df = newdf.sort_values('MAPE')\n",
        "\n",
        "n_df.to_csv(\"/content/drive/My Drive/data/bigcorn_data/제발되어라sdfsdf.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BJ88E4dws3X"
      },
      "source": [
        "d1 = pd.DataFrame(train[\"그룹코드\"])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rupgh7N1yzUt"
      },
      "source": [
        "a = train.loc[train['그룹코드'] == 1, ]\n",
        "a.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qW9O9T__Xk-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueDJooV9_Xhw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiRAsUA6_Xgw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1GQ6o8c_XeH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzFVtut8Tty8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoapTieQ_Xbu"
      },
      "source": [
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnBc8LYO_XZO"
      },
      "source": [
        "# K-Fold 지정\n",
        "K = 3\n",
        "\n",
        "# 사용하고자 하는 *모든* 변수를 입력하세요\n",
        "all_feature = ['노출(분)', '상품군', '판매단가',\n",
        "'월', '일', '시간hour',\n",
        "'계절', '168시간', '휴일', '방송시간', '매진여부',\n",
        "'sale단어', '결제수단', '한달_상품군빈도',\n",
        "'한달_중분류빈도', '한달_소분류빈도', '한달_그룹코드빈도', '기온', '강수량', '풍속', '습도', '적설',\n",
        "'전운량', '비눈여부_평균이상', '미세먼지', '초미세먼지', '판매단가순위', '상품군판매단가순위',\n",
        "'상품군요일순위', '상품군24시간순위', '중분류요일순위', '중분류24시간순위', '상품군168시간순위',\n",
        "'중분류168시간순위', '판매단가rank', '시간순위', '시간순위168', '브랜드순위', '중분류순위', '소분류순위',\n",
        "'방송내_순서', '방송순서', '판매상품종류수', '그룹코드_전체횟수', '그룹코드_대박횟수',\n",
        "'그룹코드_대박확률', '브랜드별_전체횟수', '브랜드별_대박횟수', '브랜드별_대박확률', '중분류별_전체횟수',\n",
        "'중분류별_대박횟수', '중분류별_대박확률', '소분류별_전체횟수', '소분류별_대박횟수', '소분류별_대박확률',\n",
        "'월_COS', '월_SIN', '168시간_COS', '168시간_SIN', '24시간_COS', '24시간_SIN']\n",
        "\n",
        "# PCA를 수행하고자 하는 변수를 입력하세요\n",
        "pca_feature = []\n",
        "\n",
        "# Target 변수를 입력하세요 '취급액'으로 넣어도 됩니다.\n",
        "target = ['취급액boxcox']\n",
        "\n",
        "# all_feature에 입력했던 범주형 변수를 입력하세요\n",
        "onehot_feature = ['계절', '휴일', '결제수단', '상품군', '매진여부']\n",
        "\n",
        "# 파라매터를 입력하세요\n",
        "params = {'learning_rate' : 0.1, 'num_iterations' : 1000, 'max_depth' : 6}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qekod6z_XX8"
      },
      "source": [
        "# Validataion set\n",
        "\n",
        "y_pred, y_real, temp = PCA_CV(K, train, params, all_feature, pca_feature, target, onehot_feature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0t7qQ3Q_XU1"
      },
      "source": [
        "# test set : 2019.06 예측\n",
        "\n",
        "y_pred, y_real, mape = final_predict(train, test, params, all_feature, pca_feature, target, onehot_feature)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}